https://tomdekan.com/articles/gpt-macos

-------------------------------------------------------------------------------------------

Prepare results:

tokenizer_config.json: 100%|█████████████████| 26.0/26.0 [00:00<00:00, 67.5kB/s]
vocab.json: 100%|██████████████████████████| 1.04M/1.04M [00:00<00:00, 1.61MB/s]
merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.74MB/s]
tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 2.68MB/s]
config.json: 100%|█████████████████████████████| 665/665 [00:00<00:00, 3.61MB/s]
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map:   0%|                                         | 0/1 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors
Map: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  1.55 examples/s]
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  6.59 examples/s]
Saving the dataset (1/1 shards): 100%|█| 2640/2640 [00:00<00:00, 1463516.07 exam

-------------------------------------------------------------------------------------------

Training took 50 minutes on a Mac mini M4 16 GB.
